{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# ğŸ§  Pretraining & Fine-Tuning in LLMs\n",
    "\n",
    "## ğŸ¯ Intent\n",
    "\n",
    "Train large models ğŸ“š on massive text, then adapt them for specific tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”‘ Steps\n",
    "\n",
    "* ğŸ“– **Pretraining** â†’ learn general language patterns from huge corpora (Wikipedia, books, code).\n",
    "* ğŸ¯ **Fine-Tuning** â†’ adapt to a task (sentiment, QA, summarization).\n",
    "* ğŸ”„ **Transfer Learning** â†’ reuse knowledge instead of training from scratch.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Quick Summary\n",
    "\n",
    "* ğŸ‘‰ Pretraining = general knowledge ğŸŒ.\n",
    "* ğŸ‘‰ Fine-tuning = specialize for a job ğŸ¯.\n",
    "* ğŸ‘‰ Saves time, cost & compute.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
