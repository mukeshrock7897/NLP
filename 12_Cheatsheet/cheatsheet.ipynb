{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# ğŸ“š NLP Interview Preparation Cheatsheet\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£ NLP Core Pipeline ğŸ› ï¸\n",
    "\n",
    "| ğŸ”‘ Step                   | ğŸ“Œ What itâ€™s for                      | ğŸ Python Imports & Functions                                                | ğŸ“ Notes                                          |\n",
    "| ------------------------- | ------------------------------------- | ---------------------------------------------------------------------------- | ------------------------------------------------- |\n",
    "| Data Collection ğŸŒ        | Gather raw data from APIs, web, files | `requests`, `beautifulsoup4`, `pandas`                                       | APIs, scraping, open datasets                     |\n",
    "| Preprocessing ğŸ§¹          | Clean & normalize text                | `re`, `nltk.corpus.stopwords`, `string`                                      | Lowercasing, stopwords, punctuation removal       |\n",
    "| Tokenization âœ‚ï¸           | Break text into words/subwords        | `nltk.word_tokenize`, `spacy.load().tokenizer`, `transformers.AutoTokenizer` | Rule-based vs subword BPE                         |\n",
    "| Feature Representation ğŸ§© | Convert text â†’ vectors                | `sklearn.feature_extraction.text.CountVectorizer`, `TfidfVectorizer`         | Bag-of-Words, TF-IDF, Embeddings                  |\n",
    "| Modeling ğŸ¤–               | Build ML/DL models                    | `scikit-learn`, `torch`, `tensorflow`, `transformers`                        | Classical (NB, SVM) + DL (RNN, LSTM, Transformer) |\n",
    "| Evaluation ğŸ“Š             | Measure performance                   | `sklearn.metrics` (`accuracy_score`, `f1_score`)                             | Also BLEU, ROUGE for seq2seq                      |\n",
    "| Deployment ğŸš€             | Serve models                          | `flask`, `fastapi`, `bentoml`, `torchserve`                                  | REST APIs, cloud                                  |\n",
    "| Monitoring ğŸ”             | Track drift, errors                   | `prometheus`, `mlflow`, `evidently`                                          | Essential for production                          |\n",
    "\n",
    "---\n",
    "\n",
    "## 2ï¸âƒ£ Fundamentals of Linguistics ğŸŒ\n",
    "\n",
    "| ğŸ§© Topic         | ğŸ“Œ Use in NLP                   | ğŸ Example / Notes                  |\n",
    "| ---------------- | ------------------------------- | ----------------------------------- |\n",
    "| Morphology ğŸ§¬    | Word formation (stems, affixes) | Stemming vs Lemmatization           |\n",
    "| Syntax ğŸŒ³        | Sentence structure              | Dependency parsing, POS tagging     |\n",
    "| Semantics ğŸ’¡     | Meaning representation          | Word Sense Disambiguation           |\n",
    "| Pragmatics ğŸ—£ï¸   | Contextual meaning              | Coreference resolution              |\n",
    "| Ethics & Bias âš–ï¸ | Data fairness, inclusivity      | Avoid bias in embeddings & datasets |\n",
    "\n",
    "---\n",
    "\n",
    "## 3ï¸âƒ£ Programming Tools âš™ï¸\n",
    "\n",
    "| ğŸ› ï¸ Category                | ğŸ“Œ Purpose        | ğŸ Imports / Tools                               |\n",
    "| --------------------------- | ----------------- | ------------------------------------------------ |\n",
    "| Python Essentials ğŸ        | Basic text ops    | `str.split()`, `re.sub()`, `collections.Counter` |\n",
    "| NLP Libraries ğŸ“š            | Ready pipelines   | `nltk`, `spacy`, `gensim`                        |\n",
    "| Deep Learning Frameworks ğŸ”¥ | Training models   | `torch`, `tensorflow.keras`                      |\n",
    "| Industry Tools ğŸ­           | Deployment & eval | `transformers`, `langchain`, `mlflow`            |\n",
    "\n",
    "---\n",
    "\n",
    "## 4ï¸âƒ£ Classical ML in NLP ğŸ§®\n",
    "\n",
    "| ğŸ“Œ Algorithm          | ğŸ Import / Function                                  | ğŸ” Notes                      |\n",
    "| --------------------- | ----------------------------------------------------- | ----------------------------- |\n",
    "| Naive Bayes ğŸ“Š        | `from sklearn.naive_bayes import MultinomialNB`       | Great for text classification |\n",
    "| Logistic Regression â— | `from sklearn.linear_model import LogisticRegression` | Baseline classifier           |\n",
    "| SVM âš”ï¸                | `from sklearn.svm import SVC`                         | Good for high-dim text        |\n",
    "| CRF ğŸ“‹                | `sklearn-crfsuite`                                    | Sequence labeling (NER, POS)  |\n",
    "\n",
    "---\n",
    "\n",
    "## 5ï¸âƒ£ Deep Learning in NLP ğŸ”¥\n",
    "\n",
    "| ğŸ§  Model Type   | ğŸ Imports                                       | ğŸ“ Notes                                        |\n",
    "| --------------- | ------------------------------------------------ | ----------------------------------------------- |\n",
    "| RNNs ğŸ”„         | `torch.nn.RNN`, `keras.layers.SimpleRNN`         | Handle sequences but suffer vanishing gradients |\n",
    "| LSTMs â³         | `torch.nn.LSTM`, `keras.layers.LSTM`             | Capture long-term dependencies                  |\n",
    "| GRUs âš¡          | `torch.nn.GRU`                                   | Lighter than LSTM                               |\n",
    "| CNNs for NLP ğŸŒ€ | `keras.layers.Conv1D`                            | Text classification, sentence encoding          |\n",
    "| Attention âœ¨     | `torch.nn.MultiheadAttention`                    | Focus on important tokens                       |\n",
    "| Seq2Seq ğŸ”      | `torch.nn.Transformer`, `keras.layers.Attention` | Translation, summarization                      |\n",
    "\n",
    "---\n",
    "\n",
    "## 6ï¸âƒ£ Transformers & LLMs âš¡\n",
    "\n",
    "| ğŸ“Œ Concept                    | ğŸ Imports                                  | ğŸ” Notes                                            |\n",
    "| ----------------------------- | ------------------------------------------- | --------------------------------------------------- |\n",
    "| Self-Attention ğŸ‘€             | `torch.nn.MultiheadAttention`               | Core of Transformers                                |\n",
    "| Transformer Architectures ğŸ—ï¸ | `transformers.BertModel`, `GPT2LMHeadModel` | BERT (encoder), GPT (decoder), T5 (encoder-decoder) |\n",
    "| Fine-Tuning ğŸ”§                | `Trainer` API in `transformers`             | Domain-specific adaptation                          |\n",
    "| Prompt Engineering ğŸ“         | `pipeline(\"text-generation\")`               | Few-shot, zero-shot                                 |\n",
    "| Applications ğŸ’¡               | QA, Summarization, Chatbots, RAG            | Hugging Face Pipelines                              |\n",
    "\n",
    "---\n",
    "\n",
    "## 7ï¸âƒ£ Core NLP Tasks ğŸ“\n",
    "\n",
    "| ğŸ¯ Task                   | ğŸ Tools / Imports                                              | ğŸ“ Notes                        |\n",
    "| ------------------------- | --------------------------------------------------------------- | ------------------------------- |\n",
    "| Text Classification ğŸ·ï¸   | `LogisticRegression`, `BERT`, `pipeline(\"text-classification\")` | Sentiment, topic classification |\n",
    "| Information Extraction ğŸ“‘ | `spacy.ner`, `transformers` NER models                          | NER, Relation Extraction        |\n",
    "| Text Generation âœï¸        | `GPT2LMHeadModel`, `pipeline(\"text-generation\")`                | Dialogue, story generation      |\n",
    "| Summarization ğŸ“š          | `pipeline(\"summarization\")`                                     | Abstractive vs Extractive       |\n",
    "| Machine Translation ğŸŒ    | `MarianMTModel`, `pipeline(\"translation\")`                      | Hugging Face pretrained models  |\n",
    "\n",
    "---\n",
    "\n",
    "## 8ï¸âƒ£ Evaluation Metrics ğŸ“Š\n",
    "\n",
    "| ğŸ“ Metric     | ğŸ“Œ What it measures          | ğŸ Function                      |\n",
    "| ------------- | ---------------------------- | -------------------------------- |\n",
    "| Accuracy âœ”ï¸   | Correct predictions / total  | `accuracy_score(y_true, y_pred)` |\n",
    "| Precision ğŸ¯  | Correct positive predictions | `precision_score()`              |\n",
    "| Recall ğŸ”     | % of positives captured      | `recall_score()`                 |\n",
    "| F1-Score âš–ï¸   | Harmonic mean of P & R       | `f1_score()`                     |\n",
    "| BLEU ğŸŒ       | MT quality (n-gram overlap)  | `nltk.translate.bleu_score`      |\n",
    "| ROUGE ğŸ“–      | Summarization quality        | `rouge_score`                    |\n",
    "| Perplexity ğŸ¤¯ | Language model fluency       | Lower is better                  |\n",
    "\n",
    "---\n",
    "\n",
    "## 9ï¸âƒ£ Advanced Topics ğŸš€\n",
    "\n",
    "| ğŸ§© Topic                                | ğŸ“Œ Why important            | ğŸ“ Notes                          |\n",
    "| --------------------------------------- | --------------------------- | --------------------------------- |\n",
    "| Transfer Learning ğŸ”„                    | Adapt pretrained models     | Saves data + compute              |\n",
    "| Multilingual NLP ğŸŒ                     | XLM-R, mBERT                | Cross-lingual tasks               |\n",
    "| Explainability ğŸ§                       | SHAP, LIME                  | Model interpretability            |\n",
    "| RAG (Retrieval-Augmented Generation) ğŸ” | Combine search + generation | For enterprise QA                 |\n",
    "| Agentic AI ğŸ¤–                           | Tools + reasoning           | LangChain Agents                  |\n",
    "| Ethics & Safety âš–ï¸                      | Avoid harmful outputs       | Bias mitigation, toxicity filters |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”Ÿ Python Snippets & Built-ins ğŸ\n",
    "\n",
    "| Function              | Example                   | Use                  |\n",
    "| --------------------- | ------------------------- | -------------------- |\n",
    "| `str.lower()`         | `\"Hello\".lower()`         | Normalize case       |\n",
    "| `re.sub()`            | `re.sub(r\"\\d\", \"\", text)` | Remove digits        |\n",
    "| `split()`             | `\"I love NLP\".split()`    | Tokenization (basic) |\n",
    "| `collections.Counter` | `Counter(words)`          | Word frequency       |\n",
    "| `zip()`               | `zip(words, tags)`        | Pair tokens with POS |\n",
    "| `enumerate()`         | Iterate with index        | Helpful in loops     |\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
