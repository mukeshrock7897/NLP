{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# üìöü§ñ NLP Interview Preparation Cheatsheet\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ NLP Core Pipeline (‚ÜîÔ∏é your `1_NLP_Core_Pipeline`) üõ†Ô∏è\n",
    "\n",
    "| üîë Step            | üéØ Goal                  | üêç Key Imports / Functions                                                                         | üß† Mini-Notes / Examples                                          |\n",
    "| ------------------ | ------------------------ | -------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------- |\n",
    "| Data Collection üåê | Ingest text              | `requests`, `bs4.BeautifulSoup`, `pandas.read_*`, `datasets.load_dataset`                          | APIs, scraping, CSV/JSON/Parquet; HF Datasets for benchmarks      |\n",
    "| Preprocessing üßπ   | Clean/normalize          | `re.sub`, `unicodedata.normalize`, `html.unescape`, `langdetect`                                   | Lowercasing (task-dependent), unicode NFKC, de-HTML, lang filter  |\n",
    "| Tokenization ‚úÇÔ∏è    | Split to tokens/subwords | `nltk.word_tokenize`, `spacy.load(\"en_core_web_sm\")`, `transformers.AutoTokenizer.from_pretrained` | Rule/word vs BPE/WordPiece/SentencePiece; `is_split_into_words`   |\n",
    "| Representation üß©  | Text ‚Üí vectors           | `CountVectorizer`, `TfidfVectorizer`, `gensim.models.Word2Vec`, `SentenceTransformer`              | BoW/TF-IDF; static (word2vec/GloVe/FastText); sentence embeddings |\n",
    "| Modeling ü§ñ        | Learn mapping            | `sklearn` (NB/SVM/LogReg), `torch`, `tensorflow`, `transformers`                                   | Classical baselines first; DL/Transformers for SOTA               |\n",
    "| Evaluation üìä      | Score models             | `sklearn.metrics`, `sacrebleu.corpus_bleu`, `rouge_score`                                          | Use task-appropriate metrics (see ¬ß8)                             |\n",
    "| Deployment üöÄ      | Serve safely             | `fastapi`, `uvicorn`, `bentoml`, `onnxruntime`, `torchserve`                                       | Schemas via `pydantic`; health/probe; version your models         |\n",
    "| Monitoring üîç      | Track real-world         | `mlflow`, `wandb`, `evidently`, `prometheus_client`                                                | Data/label drift, Hit@k, latency, cost, guardrails                |\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Preprocessing & Cleaning Methods (deep dive) üßΩ\n",
    "\n",
    "| üßπ Method             | üêç Snippet / Import                                    | üìù When / Why                                  |\n",
    "| --------------------- | ------------------------------------------------------ | ---------------------------------------------- |\n",
    "| Case/Whitespace       | `text.lower()`, `\" \".join(text.split())`               | Normalize only if case isn‚Äôt signal            |\n",
    "| Unicode Normalization | `unicodedata.normalize(\"NFKC\", text)`                  | Merge look-alikes; canonicalize                |\n",
    "| Punct/Digits Removal  | `re.sub(r\"[^\\w\\s]\", \"\", t)`, `re.sub(r\"\\d+\", \"\", t)`   | Be careful for dates, codes                    |\n",
    "| Stopwords             | `nltk.corpus.stopwords` / `spacy.lang.en.stop_words`   | Often helpful in BoW; less so for transformers |\n",
    "| Lemma/Stemming        | `spacy.tokenizer`+`token.lemma_`, `nltk.PorterStemmer` | Lemma preferred for readability                |\n",
    "| Contractions          | `contractions.fix(text)`                               | English normalization                          |\n",
    "| Language Detection    | `langdetect.detect(text)`                              | Filter multilingual corpora                    |\n",
    "| PII Redaction         | `presidio_analyzer`                                    | Compliance, anonymization                      |\n",
    "| Sentence Split        | `spacy.pipe`, `nltk.sent_tokenize`                     | Upstream for summarization/MT                  |\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Text Representation Techniques (‚ÜîÔ∏é `3_Feature_Representation`) üß©\n",
    "\n",
    "| üß© Technique            | üêç Import / API                                    | ‚öôÔ∏è Notes / Params                |\n",
    "| ----------------------- | -------------------------------------------------- | -------------------------------- |\n",
    "| Bag of Words            | `CountVectorizer(ngram_range=(1,2), min_df=2)`     | Sparse, fast baseline            |\n",
    "| TF-IDF                  | `TfidfVectorizer(max_features=50_000)`             | Downweights common terms         |\n",
    "| Hashing Trick           | `HashingVectorizer(n_features=2**20)`              | Memory-fixed, no vocab           |\n",
    "| Static Embeds           | `gensim.Word2Vec`, `gensim.FastText`               | OOV handling (FastText subwords) |\n",
    "| Contextual Token Embeds | `AutoModel.from_pretrained(\"bert-base-uncased\")`   | Use last-hidden states           |\n",
    "| Sentence Embeds         | `sentence_transformers` (e.g., `all-MiniLM-L6-v2`) | Semantic search, clustering      |\n",
    "| Dim. Reduction          | `TruncatedSVD`, `PCA`, `UMAP`                      | SVD for sparse TF-IDF            |\n",
    "| Pooling                 | mean/max/CLS token                                 | For token‚Üísentence aggregation   |\n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Programming Tools (‚ÜîÔ∏é `3_Programming_Tools`) ‚öôÔ∏è\n",
    "\n",
    "| üß∞ Category     | üì¶ Library / Import                               | üí° Tip                      |\n",
    "| --------------- | ------------------------------------------------- | --------------------------- |\n",
    "| Essentials      | `re`, `json`, `itertools`, `functools`, `pathlib` | Build tiny utilities fast   |\n",
    "| NLP Classics    | `nltk`, `spacy`, `gensim`                         | spaCy for fast pipelines    |\n",
    "| HF Stack        | `transformers`, `datasets`, `tokenizers`          | Unified models + datasets   |\n",
    "| Training Utils  | `accelerate`, `optuna`, `wandb`                   | Multi-GPU + HPO + tracking  |\n",
    "| Serving         | `fastapi`, `bentoml`                              | Typed endpoints w/ Pydantic |\n",
    "| Data Versioning | `dvc`, `git-lfs`                                  | Reproducible corpora        |\n",
    "| Safety          | `presidio`, moderation classifiers                | PII, toxicity checks        |\n",
    "\n",
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Classical ML in NLP (‚ÜîÔ∏é `4_Machine_Learning`) üßÆ\n",
    "\n",
    "| üßÆ Model            | üêç Import                                             | ‚úÖ Good For           | ‚ö†Ô∏è Note             |\n",
    "| ------------------- | ----------------------------------------------------- | -------------------- | ------------------- |\n",
    "| Multinomial NB      | `from sklearn.naive_bayes import MultinomialNB`       | BoW/TF-IDF text cls  | Strong baseline     |\n",
    "| Logistic Regression | `from sklearn.linear_model import LogisticRegression` | Linear separable cls | Use `max_iter` high |\n",
    "| Linear SVM          | `from sklearn.svm import LinearSVC`                   | High-dim sparse      | Robust margin       |\n",
    "| SGD (hinge/log)     | `from sklearn.linear_model import SGDClassifier`      | Large streaming      | Partial fit         |\n",
    "| CRF                 | `sklearn-crfsuite`                                    | NER/POS              | Needs hand features |\n",
    "\n",
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ Deep Learning for NLP (‚ÜîÔ∏é `5_DeepLearning`) üî•\n",
    "\n",
    "| üß† Family  | üêç Import / Layer                  | üìå Use-case     | üí° Notes                         |\n",
    "| ---------- | ---------------------------------- | --------------- | -------------------------------- |\n",
    "| RNN        | `nn.RNN`, `keras.layers.SimpleRNN` | Proto sequence  | Rarely used now                  |\n",
    "| LSTM       | `nn.LSTM`, `keras.layers.LSTM`     | Seq tagging/gen | Long deps                        |\n",
    "| GRU        | `nn.GRU`                           | Lighter LSTM    | Competitive                      |\n",
    "| CNN-Text   | `keras.layers.Conv1D`              | Classification  | N-gram features                  |\n",
    "| Attention  | `nn.MultiheadAttention`            | Focus tokens    | Pre-Transformer                  |\n",
    "| BiLSTM-CRF | `torchcrf.CRF`                     | NER             | Still strong classical-DL hybrid |\n",
    "\n",
    "---\n",
    "\n",
    "## 7Ô∏è‚É£ Transformers (‚ÜîÔ∏é `6_Transformers`) ‚ö°\n",
    "\n",
    "| üèóÔ∏è Component / Topic          | üêç Import / Tool                                  | üîß Key Params / Tips                  |\n",
    "| ------------------------------ | ------------------------------------------------- | ------------------------------------- |\n",
    "| Tokenizer                      | `AutoTokenizer`                                   | `padding`, `truncation`, `max_length` |\n",
    "| Encoder (BERT/RoBERTa/DeBERTa) | `AutoModel`, `AutoModelForSequenceClassification` | Use `CLS` pooling or mean-pool        |\n",
    "| Seq2Seq (T5/BART/Marian)       | `AutoModelForSeq2SeqLM`                           | `num_beams`, `length_penalty`         |\n",
    "| Decoder-only (GPT-style)       | `AutoModelForCausalLM`                            | `temperature`, `top_k`, `top_p`       |\n",
    "| Efficient FT                   | `peft` (LoRA/QLoRA), `bitsandbytes`               | 8/4-bit + adapters = cheap FT         |\n",
    "| Trainer API                    | `transformers.Trainer`                            | `lr`, `weight_decay`, `warmup_ratio`  |\n",
    "| Inference                      | `pipeline`, `vllm`                                | Batch, static shapes, FP16/BF16       |\n",
    "| Long Context                   | RoPE/ALiBi scaling                                | Segment packing, sliding window       |\n",
    "\n",
    "---\n",
    "\n",
    "## 8Ô∏è‚É£ LLMs & GenAI (‚ÜîÔ∏é `7_LLMs_GenAI`) üß†\n",
    "\n",
    "| üß© Technique      | üêç Library                      | üìù What to Know                         |\n",
    "| ----------------- | ------------------------------- | --------------------------------------- |\n",
    "| Pretraining       | `transformers`, `datasets`      | MLM/CLM objectives, tokenizer choice    |\n",
    "| SFT (Instruction) | `trl.SFTTrainer`                | Supervised fine-tuning on instructions  |\n",
    "| Preference Tuning | `trl` (DPO/RLHF)                | Align outputs to preferences            |\n",
    "| PEFT              | `peft`                          | LoRA/adapters prompt-tuning             |\n",
    "| Decoding          | `generate()`                    | Greedy/beam/top-k/top-p/typical         |\n",
    "| Safety            | moderation, regex/struct schema | Refuse lists, output schemas (Pydantic) |\n",
    "\n",
    "---\n",
    "\n",
    "## 9Ô∏è‚É£ Core NLP Tasks (‚ÜîÔ∏é `8_Core_Tasks`) üß±\n",
    "\n",
    "| üéØ Task                              | üêç Common APIs                                                                                | üìè Typical Metrics           | üí° Notes                         |\n",
    "| ------------------------------------ | --------------------------------------------------------------------------------------------- | ---------------------------- | -------------------------------- |\n",
    "| Classification (sentiment/topic) üè∑Ô∏è | `AutoModelForSequenceClassification`, `pipeline(\"text-classification\")`, `LogisticRegression` | Acc/Prec/Rec/F1              | Start with LogReg+TF-IDF         |\n",
    "| Sequence Labeling (NER/POS) üß∑       | `AutoModelForTokenClassification`, `spacy`                                                    | F1 (entity-level), `seqeval` | Align labels with subwords       |\n",
    "| QA (extractive/abstractive) ‚ùì        | `pipeline(\"question-answering\")`, `AutoModelForSeq2SeqLM`                                     | EM/F1, ROUGE                 | Context window limits            |\n",
    "| Summarization üìö                     | `pipeline(\"summarization\")`                                                                   | ROUGE/BERTScore              | Length control: `min_new_tokens` |\n",
    "| MT üåç                                | `MarianMTModel`, `pipeline(\"translation\")`                                                    | BLEU/ChrF/sacreBLEU          | Domain adaptation helps          |\n",
    "| Paraphrase/STS üîÅ                    | `sentence_transformers`                                                                       | Pearson/Spearman             | Cosine sim on sentence embeds    |\n",
    "| Information Extraction üìë            | `spacy.ner`, patterns, `re`                                                                   | Precision/Recall/F1          | Rule+ML hybrids practical        |\n",
    "\n",
    "---\n",
    "\n",
    "## üîü Evaluation Metrics (‚ÜîÔ∏é `5_Evaluation`) üìä\n",
    "\n",
    "| üìè Metric           | üêç Call                                    | üß† Use              |\n",
    "| ------------------- | ------------------------------------------ | ------------------- |\n",
    "| Accuracy            | `accuracy_score(y, yhat)`                  | Balanced/easy tasks |\n",
    "| Precision/Recall/F1 | `precision_recall_fscore_support`          | Imbalanced classes  |\n",
    "| AUC-ROC/PR          | `roc_auc_score`, `average_precision_score` | Prob classifiers    |\n",
    "| Confusion Matrix    | `confusion_matrix`                         | Error analysis      |\n",
    "| BLEU / sacreBLEU    | `sacrebleu.corpus_bleu`                    | MT n-gram overlap   |\n",
    "| ROUGE               | `rouge_score`                              | Summaries           |\n",
    "| BERTScore           | `bert_score.score`                         | Semantic similarity |\n",
    "| Perplexity          | `exp(loss)`                                | Language models     |\n",
    "| Ranking (IR/RAG)    | Hit@k, MRR, nDCG (`recleval`/custom)       | Retrieval quality   |\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£1Ô∏è‚É£ Multilingual & Multimodal (‚ÜîÔ∏é `9_Multimodal`) üåêüéßüñºÔ∏è\n",
    "\n",
    "| üåç/üñºÔ∏è Domain         | üêç Library                     | üß† Notes                                  |\n",
    "| --------------------- | ------------------------------ | ----------------------------------------- |\n",
    "| Multilingual Encoders | `AutoModel` (XLM-R, mBERT)     | Shared subwords; consider script coverage |\n",
    "| Translation           | `MarianMT`, `NLLB` via HF      | Use `sacrebleu` for eval                  |\n",
    "| Speech ‚Üí Text (ASR)   | `openai/whisper`, `torchaudio` | 16kHz mono, VAD helps                     |\n",
    "| Text ‚Üí Speech         | `TTS` (Coqui), `pyttsx3`       | Prosody, multilingual voices              |\n",
    "| Vision-Language       | `CLIP`, `BLIP`, `Llava`        | Cross-modal retrieval/QA                  |\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£2Ô∏è‚É£ Agentic AI & RAG (‚ÜîÔ∏é `10_Agentic_AI`) üïµÔ∏è‚Äç‚ôÇÔ∏èüß≠\n",
    "\n",
    "| üß† Component     | üêç Tooling                                   | üìù Key Ideas                                  |\n",
    "| ---------------- | -------------------------------------------- | --------------------------------------------- |\n",
    "| Retrieval        | `faiss`, `chromadb`, `sentence_transformers` | Chunking, overlap, hybrid search (BM25+dense) |\n",
    "| Generators       | `AutoModelForSeq2SeqLM` / CausalLM           | Grounding + citations                         |\n",
    "| Orchestration    | `langchain`, `langgraph`                     | Tools, planning (ReAct, Plan&Execute)         |\n",
    "| Evaluation (RAG) | `ragas`                                      | Faithfulness, answer relevance                |\n",
    "| Caching          | `redis` / in-proc                            | Cut cost/latency; TTL by prompt hash          |\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£3Ô∏è‚É£ Scalability, Prod Readiness & Trends (‚ÜîÔ∏é `11_Scalability_Trends`) üèóÔ∏è\n",
    "\n",
    "| üè≠ Topic              | üîß Stack                                         | üí° Practical Tip              |\n",
    "| --------------------- | ------------------------------------------------ | ----------------------------- |\n",
    "| Packaging             | `poetry`, `uv`, `Docker`                         | Pin models/tokenizers         |\n",
    "| Serving at Scale      | `vllm`, TGI, `ray`                               | Throughput via paged KV cache |\n",
    "| Observability         | `mlflow`, `wandb`, `prometheus`, `opentelemetry` | Trace‚Üíspan LLM calls          |\n",
    "| Cost Controls         | batching, quant (8/4-bit), early-exit            | Log tokens & latency          |\n",
    "| Safety & Ethics       | policy checks, `presidio`, curated red teams     | Pre-deploy red teaming        |\n",
    "| Data/Model Versioning | `dvc`, model cards                               | Always log config + seed      |\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£4Ô∏è‚É£ Python Imports & Built-ins (with tiny examples) üêç\n",
    "\n",
    "| üîß Item                 | ‚ú® Example                         | üß† Use               |\n",
    "| ----------------------- | --------------------------------- | -------------------- |\n",
    "| `re.sub`                | `re.sub(r\"\\s+\", \" \", t)`          | Whitespace collapse  |\n",
    "| `unicodedata.normalize` | `normalize(\"NFKC\", t)`            | Unicode tidy         |\n",
    "| `collections.Counter`   | `Counter(tokens).most_common(10)` | Top-k words          |\n",
    "| `itertools.islice`      | `list(islice(iterable, 100))`     | Take first N         |\n",
    "| `functools.lru_cache`   | `@lru_cache`                      | Cache heavy funcs    |\n",
    "| `pathlib.Path.glob`     | `Path(\"data\").glob(\"*.txt\")`      | File discovery       |\n",
    "| `json.loads/dumps`      | `json.loads(s)`                   | Data interchange     |\n",
    "| `textwrap.shorten`      | `shorten(t, width=120)`           | Console summaries    |\n",
    "| `argparse`              | CLI flags                         | Reproducible scripts |\n",
    "| `typing` + `pydantic`   | Typed DTOs                        | Safer I/O schemas    |\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£5Ô∏è‚É£ Quick Hugging Face Patterns üß™\n",
    "\n",
    "| üéØ Task       | üêç One-liners                                                                  | üí° Notes                                                 |\n",
    "| ------------- | ------------------------------------------------------------------------------ | -------------------------------------------------------- |\n",
    "| Tokenizer     | `tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")`                     | `padding=\"max_length\"` for batching                      |\n",
    "| Text Cls      | `pipe = pipeline(\"text-classification\", model=...)`                            | For production, use `AutoModelForSequenceClassification` |\n",
    "| Summarization | `pipeline(\"summarization\")(text, max_new_tokens=128)`                          | Control length                                           |\n",
    "| Generation    | `model.generate(**tok(text, return_tensors=\"pt\"), top_p=0.9, temperature=0.7)` | Sampling configs                                         |\n",
    "| Seq2Seq FT    | `Trainer(model, args, train_dataset, eval_dataset)`                            | Log metrics; early stopping                              |\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£6Ô∏è‚É£ Interview Tip Cards üíº\n",
    "\n",
    "| üß† Topic         | üí° What to Say in 20s                                                |\n",
    "| ---------------- | -------------------------------------------------------------------- |\n",
    "| Baselines First  | ‚ÄúI start with TF-IDF + LogReg; then move to Transformers if needed.‚Äù |\n",
    "| Data Matters     | ‚ÄúI invest in cleaning, label quality, and ablations before tuning.‚Äù  |\n",
    "| Metrics Fit Task | ‚ÄúFor seq2seq I use ROUGE/BERTScore; retrieval uses MRR/nDCG.‚Äù        |\n",
    "| Efficient FT     | ‚ÄúPEFT (LoRA/QLoRA) + 4-bit quant drastically reduces cost.‚Äù          |\n",
    "| Guardrails       | ‚ÄúTyped schemas (Pydantic), PII redaction, moderation, and logging.‚Äù  |\n",
    "| Observability    | ‚ÄúI track latency, token counts, drift (Evidently), and cost.‚Äù        |\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
